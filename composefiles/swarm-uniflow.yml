# UniFlow containers
version: "3.7"

x-logging: &logging_loki
    driver: loki
    options:
      loki-url: "http://${MANAGER_IP_ADDR}:3100/loki/api/v1/push"
      loki-retries: "1"
      loki-batch-size: "1500"
      loki-relabel-config: |
        - action: labelmap
          regex: swarm_(service)

x-logging: &logging_json
    driver: "json-file"
    options:
      max-file: "3"
      max-size: "10m"

services:
  conductor-server:
    image: frinx/uniflow-conductor-server:1.0.1
    # user: guest
    logging: *logging_loki
    labels:
      - traefik.enable=false  
    environment:
      - CONFIG_PROP=config.properties
      - _JAVA_OPTIONS="-Xmx${CS_RES_LIMIT_MEM}"
    volumes:
      - ${UF_CONFIG_PATH}/conductor/config.properties:/app/config/config.properties:ro
    healthcheck:
      test: curl -I -XGET http://localhost:8080/health
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 60s
    ulimits:
      nofile:
        soft: ${CS_ULIMIT_NOFILE_SOFT}
        hard: ${CS_ULIMIT_NOFILE_HARD}
      nproc:
        soft: ${CS_ULIMIT_NPROC_SOFT}
        hard: ${CS_ULIMIT_NPROC_HARD}
    cap_drop:
      - all
    sysctls:
      net.ipv4.tcp_keepalive_time: 300
    deploy:
      placement:
        constraints:
          - node.role == manager
      restart_policy:
        condition: on-failure
        delay: 5s
        window: 10s
      mode: replicated
      replicas: 1
      resources:
        limits:
          cpus: ${CS_RES_LIMIT_CPUS}
          memory: ${CS_RES_LIMIT_MEM}


  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:6.7.1
    user: elasticsearch
    logging: *logging_loki
    labels:
      - traefik.enable=false 
    environment:
      - "discovery.type=single-node"
      - "ES_JAVA_OPTS=-Xms512m -Xmx3584m"
    volumes:
      - frinx_uniflow_elastic_data:/usr/share/elasticsearch/data
      - ${UF_CONFIG_PATH}/elasticsearch/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml:ro
      - ${UF_CONFIG_PATH}/elasticsearch/backup.sh:/usr/share/elasticsearch/backup.sh:ro
    healthcheck:
      test: curl -X GET 'localhost:9200/_cluster/health?wait_for_status=yellow&timeout=5s'
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s
    ulimits:
      nofile:
        soft: ${ES_ULIMIT_NOFILE_SOFT}
        hard: ${ES_ULIMIT_NOFILE_HARD}
      nproc:
        soft: ${ES_ULIMIT_NPROC_SOFT}
        hard: ${ES_ULIMIT_NPROC_HARD}
    cap_drop:
      - all
    deploy:
      placement:
        constraints:
          - node.role == manager
      restart_policy:
        condition: on-failure
        delay: 5s
        window: 10s
      mode: replicated
      replicas: 1
      resources:
        limits:
          cpus: ${ES_RES_LIMIT_CPUS}
          memory: ${ES_RES_LIMIT_MEM}

  frinx-frontend:
    image: frinx/frinx-frontend:1.0.1
    logging: *logging_loki
    labels:
      - traefik.enable=false  
    volumes:
      - ${UF_CONFIG_PATH}/frinx-frontend/config.json:/usr/share/nginx/html/config.json:ro
    healthcheck:
      test: curl --silent -o /dev/null  --write-out 'HTTPSTATUS:%{http_code}\n' -X GET 'http://127.0.0.1:8888' || exit 1
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    ulimits:
      nofile:
        soft: ${FF_ULIMIT_NOFILE_SOFT}
        hard: ${FF_ULIMIT_NOFILE_HARD}
      nproc:
        soft: ${FF_ULIMIT_NPROC_SOFT}
        hard: ${FF_ULIMIT_NPROC_HARD}
    cap_drop:
      - all
    deploy:
      placement:
        constraints:
          - node.role == manager
      restart_policy:
        condition: on-failure
        delay: 5s
        window: 10s
      mode: replicated
      replicas: 1
      resources:
        limits:
          cpus: ${FF_RES_LIMIT_CPUS}
          memory: ${FF_RES_LIMIT_MEM}

  postgresql:
    image: postgres:12.2
    user: postgres
    logging: *logging_loki
    labels:
      - traefik.enable=false  
    environment:
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_USER=postgres
      - POSTGRES_MULTIPLE_DATABASES=conductor,frinx,schellar
    volumes:
      - frinx_uniflow_postgresql_data:/var/lib/postgresql/data
      - ${UF_CONFIG_PATH}/uniflow-postgres:/docker-entrypoint-initdb.d:ro
    healthcheck:
      test: pg_isready -U postgres
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    ulimits:
      nofile:
        soft: ${PG_ULIMIT_NOFILE_SOFT}
        hard: ${PG_ULIMIT_NOFILE_HARD}
      nproc:
        soft: ${PG_ULIMIT_NPROC_SOFT}
        hard: ${PG_ULIMIT_NPROC_HARD}
    cap_drop:
      - all
    deploy:
      placement:
        constraints:
          - node.role == manager
      restart_policy:
        condition: on-failure
        delay: 5s
        window: 10s
      mode: replicated
      replicas: 1
      resources:
        limits:
          cpus: ${PG_RES_LIMIT_CPUS}
          memory: ${PG_RES_LIMIT_MEM}

  schellar:
    image: frinx/uniflow-schellar:1.9.3
    logging: *logging_loki
    labels:
      - traefik.enable=false  
    environment:
      - LOG_LEVEL=debug
      - CHECK_INTERVAL_SECONDS=10
      - CONDUCTOR_API_URL=http://conductor-server:8080/api
      - BACKEND=postgres
      - POSTGRES_MIGRATIONS_DIR=migrations
      - POSTGRES_DATABASE_URL=host=postgresql port=5432 user=postgres password=postgres database=schellar
    healthcheck:
      test: wget --spider -q conductor-server:8080/health && wget --spider -q 127.0.0.1:3000/liveness && nc -z postgresql:5432
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s
    ulimits:
      nofile:
        soft: ${SC_ULIMIT_NOFILE_SOFT}
        hard: ${SC_ULIMIT_NOFILE_HARD}
      nproc:
        soft: ${SC_ULIMIT_NPROC_SOFT}
        hard: ${SC_ULIMIT_NPROC_HARD}
    cap_drop:
      - all
    deploy:
      placement:
        constraints:
          - node.role == manager
      restart_policy:
        condition: on-failure
        delay: 5s
        window: 10s
      mode: replicated
      replicas: 1
      resources:
        limits:
          cpus: ${SC_RES_LIMIT_CPUS}
          memory: ${SC_RES_LIMIT_MEM}

  workflow-proxy:
    image: frinx/workflow-proxy:1.0.5
    user: node
    logging: *logging_loki
    labels:
      - traefik.enable=false  
    healthcheck:
      test: curl --silent --write-out 'HTTPSTATUS:%{http_code}' -X GET 'http://127.0.0.1:8088/probe/readiness' && curl -I -XGET http://conductor-server:8080/health || exit 1
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s
    ulimits:
      nofile:
        soft: ${WP_ULIMIT_NOFILE_SOFT}
        hard: ${WP_ULIMIT_NOFILE_HARD}
      nproc:
        soft: ${WP_ULIMIT_NPROC_SOFT}
        hard: ${WP_ULIMIT_NPROC_HARD}
    cap_drop:
      - all
    deploy:
      placement:
        constraints:
          - node.role == manager
      restart_policy:
        condition: on-failure
        delay: 5s
        window: 10s
      mode: replicated
      replicas: 1
      resources:
        limits:
          cpus: ${WP_RES_LIMIT_CPUS}
          memory: ${WP_RES_LIMIT_MEM}

  uniresource:
    image: frinx/resource-manager:1.0.1
    logging: *logging_loki
    labels:
      - traefik.enable=false  
    environment:
      - RM_DB_CONNECTION_STRING=postgres://postgres:postgres@postgresql:5432/postgres?sslmode=disable
      - RM_API_PORT=8884
      - RM_ADMIN_ROLES=OWNER
      - RM_ADMIN_GROUPS=network-admin
      - RM_LOG_PATH=/var/log/rm.log
      - RM_LOG_LEVEL=warn
      - WASMER_MAX_TIMEOUT_MILLIS=10000
    healthcheck:
      test: curl --silent --write-out 'HTTPSTATUS:%{http_code}' -X GET 'http://0.0.0.0:8884/healthz/readiness'
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s
    ulimits:
      nofile:
        soft: ${UR_ULIMIT_NOFILE_SOFT}
        hard: ${UR_ULIMIT_NOFILE_HARD}
      nproc:
        soft: ${UR_ULIMIT_NPROC_SOFT}
        hard: ${UR_ULIMIT_NPROC_HARD}
    cap_drop:
      - all
    deploy:
      placement:
        constraints:
          - node.role == manager
      restart_policy:
        condition: on-failure
        delay: 5s
        window: 10s
      mode: replicated
      replicas: 1
      resources:
        limits:
          cpus: ${UR_RES_LIMIT_CPUS}
          memory: ${UR_RES_LIMIT_MEM}

  inventory:
    image: frinx/frinx-inventory-server:1.0.0
    logging: *logging_loki
    labels:
      - traefik.enable=false  
    environment:
      - DATABASE_URL=postgres://postgres:postgres@inventory-postgres:5432/inventory?sslmode=disable
      - UNICONFIG_API_PROTOCOL=https
      - UNICONFIG_API_PORT=8181
      - UNICONFIG_LIST_URL=http://krakend:8080/static/list/uniconfig
    volumes:
      - ${UF_CONFIG_PATH}/inventory/run_inventory.sh:/run_inventory.sh:ro
    healthcheck:
      test: wget --spider -q '0.0.0.0:8000/.well-known/apollo/server-health'
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 40s
    entrypoint: ["/run_inventory.sh"]
    ulimits:
      nofile:
        soft: ${IV_ULIMIT_NOFILE_SOFT}
        hard: ${IV_ULIMIT_NOFILE_HARD}
      nproc:
        soft: ${IV_ULIMIT_NPROC_SOFT}
        hard: ${IV_ULIMIT_NPROC_HARD}
    cap_drop:
      - all
    deploy:
      placement:
        constraints:
          - node.role == manager
      mode: replicated
      replicas: 1
      resources:
        limits:
          cpus: ${IV_RES_LIMIT_CPUS}
          memory: ${IV_RES_LIMIT_MEM}

  inventory-postgres:
    image: postgres:12.2
    user: postgres
    logging: *logging_loki
    labels:
      - traefik.enable=false  
    environment:
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_USER=postgres
      - POSTGRES_DB=inventory
    volumes:
      - frinx_uniflow_inventory_postgresql_data:/var/lib/postgresql/data
    healthcheck:
      test: pg_isready -U postgres
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    ulimits:
      nofile:
        soft: ${IP_ULIMIT_NOFILE_SOFT}
        hard: ${IP_ULIMIT_NOFILE_HARD}
      nproc:
        soft: ${IP_ULIMIT_NPROC_SOFT}
        hard: ${IP_ULIMIT_NPROC_HARD}
    cap_drop:
      - all
    deploy:
      placement:
        constraints:
          - node.role == manager
      restart_policy:
        condition: on-failure
        delay: 5s
        window: 10s
      mode: replicated
      replicas: 1
      resources:
        limits:
          cpus: ${IP_RES_LIMIT_CPUS}
          memory: ${IP_RES_LIMIT_MEM}


volumes:
  frinx_uniflow_elastic_data:
    name: frinx_uniflow_elastic_data
  frinx_uniflow_postgresql_data:
    name: frinx_uniflow_postgresql_data
  frinx_uniflow_inventory_postgresql_data:
    name: frinx_uniflow_inventory_postgresql_data

networks:
  default:
    name: frinx-machine
