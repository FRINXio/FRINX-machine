# UniFlow containers
version: "3.7"

services:

  conductor-server:
    image: frinx/uniflow-conductor-server:latest
    # user: guest
    logging:
      driver: "json-file"
      options:
        max-file: "3"
        max-size: "10m"
    environment:
      - CONFIG_PROP=config.properties
      - _JAVA_OPTIONS="-Xmx${CS_RES_LIMIT_MEM}"
    volumes:
      - ${UF_CONFIG_PATH}/conductor/config.properties:/app/config/config.properties:ro
    healthcheck:
      test: curl -I -XGET http://localhost:8080/health
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 60s
    ulimits:
      nofile:
        soft: ${CS_ULIMIT_NOFILE_SOFT}
        hard: ${CS_ULIMIT_NOFILE_HARD}
      nproc:
        soft: ${CS_ULIMIT_NPROC_SOFT}
        hard: ${CS_ULIMIT_NPROC_HARD}
    cap_drop:
      - all
    deploy:
      placement:
        constraints:
          - node.id == ${UF_SWARM_NODE_ID}
      restart_policy:
        condition: on-failure
        delay: 5s
        window: 10s
      mode: replicated
      replicas: 1
      resources:
        limits:
          cpus: ${CS_RES_LIMIT_CPUS}
          memory: ${CS_RES_LIMIT_MEM}


  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:6.7.1
    user: elasticsearch
    logging:
      driver: "json-file"
      options:
        max-file: "3"
        max-size: "10m"
    environment:
      - "discovery.type=single-node"
      - "ES_JAVA_OPTS=-Xms512m -Xmx3584m"
    volumes:
      - uniflow_elastic_data:/usr/share/elasticsearch/data
      - ${UF_CONFIG_PATH}/elasticsearch/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml:ro
      - ${UF_CONFIG_PATH}/elasticsearch/backup.sh:/usr/share/elasticsearch/backup.sh:ro
    healthcheck:
      test: curl -X GET 'localhost:9200/_cluster/health?wait_for_status=yellow&timeout=5s'
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s
    ulimits:
      nofile:
        soft: ${ES_ULIMIT_NOFILE_SOFT}
        hard: ${ES_ULIMIT_NOFILE_HARD}
      nproc:
        soft: ${ES_ULIMIT_NPROC_SOFT}
        hard: ${ES_ULIMIT_NPROC_HARD}
    cap_drop:
      - all
    deploy:
      placement:
        constraints:
          - node.id == ${UF_SWARM_NODE_ID}
      restart_policy:
        condition: on-failure
        delay: 5s
        window: 10s
      mode: replicated
      replicas: 1
      resources:
        limits:
          cpus: ${ES_RES_LIMIT_CPUS}
          memory: ${ES_RES_LIMIT_MEM}

  frinx-frontend:
    image: frinx/frinx-frontend:latest
    logging:
      driver: "json-file"
      options:
        max-file: "3"
        max-size: "10m"
    volumes:
      - ${UF_CONFIG_PATH}/frinx-frontend/config.json:/usr/share/nginx/html/config.json:ro
    healthcheck:
      test: curl --silent -o /dev/null  --write-out 'HTTPSTATUS:%{http_code}\n' -X GET 'http://127.0.0.1:8888' || exit 1
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    ulimits:
      nofile:
        soft: ${FF_ULIMIT_NOFILE_SOFT}
        hard: ${FF_ULIMIT_NOFILE_HARD}
      nproc:
        soft: ${FF_ULIMIT_NPROC_SOFT}
        hard: ${FF_ULIMIT_NPROC_HARD}
    cap_drop:
      - all
    deploy:
      placement:
        constraints:
          - node.id == ${UF_SWARM_NODE_ID}
      restart_policy:
        condition: on-failure
        delay: 5s
        window: 10s
      mode: replicated
      replicas: 1
      resources:
        limits:
          cpus: ${FF_RES_LIMIT_CPUS}
          memory: ${FF_RES_LIMIT_MEM}

  postgresql:
    image: postgres:12.2
    user: postgres
    logging:
      driver: "json-file"
      options:
        max-file: "3"
        max-size: "10m"
    environment:
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_USER=postgres
      - POSTGRES_MULTIPLE_DATABASES=conductor,frinx,schellar
    volumes:
      - uniflow_postgresql_data:/var/lib/postgresql/data
      - ${UF_CONFIG_PATH}/uniflow-postgres:/docker-entrypoint-initdb.d:ro
    healthcheck:
      test: pg_isready -U postgres
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    ulimits:
      nofile:
        soft: ${PG_ULIMIT_NOFILE_SOFT}
        hard: ${PG_ULIMIT_NOFILE_HARD}
      nproc:
        soft: ${PG_ULIMIT_NPROC_SOFT}
        hard: ${PG_ULIMIT_NPROC_HARD}
    cap_drop:
      - all
    deploy:
      placement:
        constraints:
          - node.id == ${UF_SWARM_NODE_ID}
      restart_policy:
        condition: on-failure
        delay: 5s
        window: 10s
      mode: replicated
      replicas: 1
      resources:
        limits:
          cpus: ${PG_RES_LIMIT_CPUS}
          memory: ${PG_RES_LIMIT_MEM}

  schellar:
    image: frinx/uniflow-schellar:1.9.2
    user: guest
    logging:
      driver: "json-file"
      options:
        max-file: "3"
        max-size: "10m"
    environment:
      - LOG_LEVEL=debug
      - CHECK_INTERVAL_SECONDS=10
      - CONDUCTOR_API_URL=http://conductor-server:8080/api
      - BACKEND=postgres
      - POSTGRES_MIGRATIONS_DIR=migrations
      - POSTGRES_DATABASE_URL=host=postgresql port=5432 user=postgres password=postgres database=schellar
    healthcheck:
      test: wget --spider -q conductor-server:8080/health && wget --spider -q 127.0.0.1:3000/schedule && nc -z postgresql:5432
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s
    ulimits:
      nofile:
        soft: ${SC_ULIMIT_NOFILE_SOFT}
        hard: ${SC_ULIMIT_NOFILE_HARD}
      nproc:
        soft: ${SC_ULIMIT_NPROC_SOFT}
        hard: ${SC_ULIMIT_NPROC_HARD}
    cap_drop:
      - all
    deploy:
      placement:
        constraints:
          - node.id == ${UF_SWARM_NODE_ID}
      restart_policy:
        condition: on-failure
        delay: 5s
        window: 10s
      mode: replicated
      replicas: 1
      resources:
        limits:
          cpus: ${SC_RES_LIMIT_CPUS}
          memory: ${SC_RES_LIMIT_MEM}

  workflow-proxy:
    image: frinx/workflow-proxy:latest
    user: node
    logging:
      driver: "json-file"
      options:
        max-file: "3"
        max-size: "10m"
    healthcheck:
      test: curl --silent --write-out 'HTTPSTATUS:%{http_code}' -X GET 'http://127.0.0.1:8088/probe/readiness' && curl -I -XGET http://conductor-server:8080/health || exit 1
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s
    ulimits:
      nofile:
        soft: ${WP_ULIMIT_NOFILE_SOFT}
        hard: ${WP_ULIMIT_NOFILE_HARD}
      nproc:
        soft: ${WP_ULIMIT_NPROC_SOFT}
        hard: ${WP_ULIMIT_NPROC_HARD}
    cap_drop:
      - all
    deploy:
      placement:
        constraints:
          - node.id == ${UF_SWARM_NODE_ID}
      restart_policy:
        condition: on-failure
        delay: 5s
        window: 10s
      mode: replicated
      replicas: 1
      resources:
        limits:
          cpus: ${WP_RES_LIMIT_CPUS}
          memory: ${WP_RES_LIMIT_MEM}

  uniresource:
    image: frinx/resource-manager:1.0.0
    logging:
      driver: "json-file"
      options:
        max-file: "3"
        max-size: "10m"
    environment:
      - RM_DB_CONNECTION_STRING=postgres://postgres:postgres@postgresql:5432/postgres?sslmode=disable
      - RM_API_PORT=8884
      - RM_ADMIN_ROLES=OWNER
      - RM_ADMIN_GROUPS=network-admin
      - RM_LOG_PATH=/var/log/rm.log
      - RM_LOG_LEVEL=warn
      - WASMER_MAX_TIMEOUT_MILLIS=10000
    healthcheck:
      test: curl --silent --write-out 'HTTPSTATUS:%{http_code}' -X GET 'http://0.0.0.0:8884/healthz/readiness'
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s
    ulimits:
      nofile:
        soft: ${UR_ULIMIT_NOFILE_SOFT}
        hard: ${UR_ULIMIT_NOFILE_HARD}
      nproc:
        soft: ${UR_ULIMIT_NPROC_SOFT}
        hard: ${UR_ULIMIT_NPROC_HARD}
    cap_drop:
      - all
    deploy:
      placement:
        constraints:
          - node.id == ${UF_SWARM_NODE_ID}
      restart_policy:
        condition: on-failure
        delay: 5s
        window: 10s
      mode: replicated
      replicas: 1
      resources:
        limits:
          cpus: ${UR_RES_LIMIT_CPUS}
          memory: ${UR_RES_LIMIT_MEM}
      
volumes:
  uniflow_elastic_data:
    name: uniflow_elastic_data
  uniflow_postgresql_data:
    name: uniflow_postgresql_data

networks:
  default:
    name: frinx-machine
